"""Initial OEWS migration schema

Revision ID: 339b7f1f5ff3
Revises: 
Create Date: 2025-10-04 13:33:15.659473

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '339b7f1f5ff3'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('audit_log',
    sa.Column('operation_type', sa.String(length=50), nullable=False, comment='Type of operation: file_discovery, schema_analysis, migration, validation, rollback'),
    sa.Column('entity_type', sa.String(length=50), nullable=False, comment='Type of entity affected: excel_file, migration_batch, validation_report, etc.'),
    sa.Column('entity_id', sa.UUID(), nullable=True, comment='ID of the affected entity'),
    sa.Column('operation_name', sa.String(length=255), nullable=False, comment='Human-readable operation name'),
    sa.Column('operation_status', sa.String(length=20), nullable=False, comment='Operation result: success, failure, warning'),
    sa.Column('user_context', sa.String(length=255), nullable=True, comment='User or system context that initiated the operation'),
    sa.Column('session_id', sa.String(length=100), nullable=True, comment='Session or batch ID for grouping related operations'),
    sa.Column('changes_made', postgresql.JSON(astext_type=Text()), nullable=True, comment='Detailed list of changes made during the operation'),
    sa.Column('old_values', postgresql.JSON(astext_type=Text()), nullable=True, comment='Previous values before the operation'),
    sa.Column('new_values', postgresql.JSON(astext_type=Text()), nullable=True, comment='New values after the operation'),
    sa.Column('error_message', sa.Text(), nullable=True, comment='Error message if operation failed'),
    sa.Column('execution_time_ms', sa.Integer(), nullable=True, comment='Operation execution time in milliseconds'),
    sa.Column('memory_usage_mb', sa.Integer(), nullable=True, comment='Peak memory usage during operation in MB'),
    sa.Column('operation_metadata', postgresql.JSON(astext_type=Text()), nullable=True, comment='Additional operation metadata and context'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("operation_status IN ('success', 'failure', 'warning', 'in_progress')", name=op.f('ck_audit_log_valid_operation_status')),
    sa.CheckConstraint("operation_type IN ('file_discovery', 'schema_analysis', 'migration', 'validation', 'rollback', 'system')", name=op.f('ck_audit_log_valid_operation_type')),
    sa.CheckConstraint('execution_time_ms IS NULL OR execution_time_ms >= 0', name=op.f('ck_audit_log_valid_execution_time')),
    sa.CheckConstraint('memory_usage_mb IS NULL OR memory_usage_mb >= 0', name=op.f('ck_audit_log_valid_memory_usage')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_audit_log'))
    )
    op.create_table('excel_file',
    sa.Column('file_path', sa.String(length=500), nullable=False, comment='Absolute path to the Excel file'),
    sa.Column('file_name', sa.String(length=255), nullable=False, comment='Base name of the file'),
    sa.Column('file_size', sa.Integer(), nullable=False, comment='Size in bytes'),
    sa.Column('file_hash', sa.String(length=64), nullable=False, comment='SHA-256 hash for change detection'),
    sa.Column('sheet_count', sa.Integer(), nullable=True, comment='Number of worksheets in the file'),
    sa.Column('file_created_at', sa.DateTime(timezone=True), nullable=True, comment='File creation timestamp from filesystem'),
    sa.Column('file_modified_at', sa.DateTime(timezone=True), nullable=True, comment='File last modification timestamp from filesystem'),
    sa.Column('processed_at', sa.DateTime(timezone=True), nullable=True, comment='When migration processing began'),
    sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True, comment='When migration processing completed'),
    sa.Column('status', sa.String(length=20), nullable=False, comment='Current processing status'),
    sa.Column('error_message', sa.String(length=1000), nullable=True, comment='Error message if processing failed'),
    sa.Column('is_oews_format', sa.Boolean(), nullable=True, comment='Whether file matches expected OEWS format'),
    sa.Column('oews_year', sa.Integer(), nullable=True, comment='OEWS data year extracted from filename or content'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("status IN ('discovered', 'analyzing', 'migrating', 'completed', 'failed', 'rolled_back')", name=op.f('ck_excel_file_valid_status')),
    sa.CheckConstraint('completed_at IS NULL OR processed_at IS NULL OR completed_at >= processed_at', name=op.f('ck_excel_file_valid_processing_timeline')),
    sa.CheckConstraint('file_size > 0 AND file_size <= 104857600', name=op.f('ck_excel_file_valid_file_size')),
    sa.CheckConstraint('oews_year IS NULL OR (oews_year >= 2000 AND oews_year <= 2030)', name=op.f('ck_excel_file_valid_oews_year')),
    sa.CheckConstraint('sheet_count IS NULL OR sheet_count >= 0', name=op.f('ck_excel_file_valid_sheet_count')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_excel_file')),
    sa.UniqueConstraint('file_hash', name=op.f('uq_excel_file_file_hash')),
    sa.UniqueConstraint('file_path', name=op.f('uq_excel_file_file_path'))
    )
    op.create_table('unified_schema',
    sa.Column('schema_name', sa.String(length=100), nullable=False, comment='Name of the unified schema'),
    sa.Column('description', sa.Text(), nullable=True, comment='Description of the schema purpose and scope'),
    sa.Column('version', sa.String(length=20), nullable=False, comment='Schema version for change tracking'),
    sa.Column('table_definitions', postgresql.JSON(astext_type=Text()), nullable=False, comment='Complete table and column definitions as JSON'),
    sa.Column('source_files_count', sa.Integer(), nullable=False, comment='Number of Excel files this schema accommodates'),
    sa.Column('total_columns', sa.Integer(), nullable=True, comment='Total number of columns across all tables'),
    sa.Column('oews_years_covered', postgresql.JSON(astext_type=Text()), nullable=True, comment='Array of OEWS data years this schema covers'),
    sa.Column('is_oews_compliant', sa.Boolean(), nullable=False, comment='Whether schema follows OEWS standard structure'),
    sa.Column('status', sa.String(length=20), nullable=False, comment='Schema status: draft, validated, deployed, deprecated'),
    sa.Column('is_active', sa.Boolean(), nullable=False, comment='Whether this schema is currently active'),
    sa.Column('validation_errors', postgresql.JSON(astext_type=Text()), nullable=True, comment='Schema validation errors and warnings'),
    sa.Column('compatibility_score', sa.Integer(), nullable=True, comment='Schema compatibility score (0-100)'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("status IN ('draft', 'validated', 'deployed', 'deprecated')", name=op.f('ck_unified_schema_valid_status')),
    sa.CheckConstraint('compatibility_score IS NULL OR (compatibility_score >= 0 AND compatibility_score <= 100)', name=op.f('ck_unified_schema_valid_compatibility_score')),
    sa.CheckConstraint('source_files_count >= 0', name=op.f('ck_unified_schema_valid_source_files_count')),
    sa.CheckConstraint('total_columns IS NULL OR total_columns > 0', name=op.f('ck_unified_schema_valid_total_columns')),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_unified_schema')),
    sa.UniqueConstraint('schema_name', name=op.f('uq_unified_schema_schema_name'))
    )
    op.create_table('excel_sheet',
    sa.Column('excel_file_id', sa.UUID(), nullable=False, comment='Foreign key to ExcelFile'),
    sa.Column('sheet_name', sa.String(length=255), nullable=False, comment='Name of the worksheet'),
    sa.Column('sheet_index', sa.Integer(), nullable=False, comment='Zero-based index within the workbook'),
    sa.Column('row_count', sa.Integer(), nullable=True, comment='Total number of data rows'),
    sa.Column('column_count', sa.Integer(), nullable=True, comment='Total number of columns'),
    sa.Column('header_row', sa.Integer(), nullable=False, comment='Row number containing column headers'),
    sa.Column('data_start_row', sa.Integer(), nullable=False, comment='First row containing actual data'),
    sa.Column('has_header', sa.Boolean(), nullable=False, comment='Whether the sheet has column headers'),
    sa.Column('inferred_schema', postgresql.JSON(astext_type=Text()), nullable=True, comment='Detected column types and relationships as JSON'),
    sa.Column('is_data_sheet', sa.Boolean(), nullable=True, comment='Whether this is the main data sheet (vs metadata sheet)'),
    sa.Column('oews_sheet_type', sa.String(length=50), nullable=True, comment='Type of OEWS sheet: data, field_descriptions, update_time, filler'),
    sa.Column('empty_rows', sa.Integer(), nullable=True, comment='Number of completely empty rows'),
    sa.Column('empty_columns', sa.Integer(), nullable=True, comment='Number of completely empty columns'),
    sa.Column('data_density', sa.String(length=10), nullable=True, comment='Percentage of cells with data'),
    sa.Column('analyzed_at', sa.String(length=30), nullable=True, comment='When schema analysis was performed'),
    sa.Column('analysis_errors', sa.Text(), nullable=True, comment='Any errors encountered during analysis'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("oews_sheet_type IS NULL OR oews_sheet_type IN ('data', 'field_descriptions', 'update_time', 'filler')", name=op.f('ck_excel_sheet_valid_oews_sheet_type')),
    sa.CheckConstraint('column_count IS NULL OR column_count >= 0', name=op.f('ck_excel_sheet_valid_column_count')),
    sa.CheckConstraint('data_start_row >= 0', name=op.f('ck_excel_sheet_valid_data_start_row')),
    sa.CheckConstraint('empty_columns IS NULL OR empty_columns >= 0', name=op.f('ck_excel_sheet_valid_empty_columns')),
    sa.CheckConstraint('empty_rows IS NULL OR empty_rows >= 0', name=op.f('ck_excel_sheet_valid_empty_rows')),
    sa.CheckConstraint('header_row >= 0', name=op.f('ck_excel_sheet_valid_header_row')),
    sa.CheckConstraint('row_count IS NULL OR row_count >= 0', name=op.f('ck_excel_sheet_valid_row_count')),
    sa.CheckConstraint('sheet_index >= 0', name=op.f('ck_excel_sheet_valid_sheet_index')),
    sa.ForeignKeyConstraint(['excel_file_id'], ['excel_file.id'], name=op.f('fk_excel_sheet_excel_file_id_excel_file'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_excel_sheet'))
    )
    op.create_table('migration_batch',
    sa.Column('unified_schema_id', sa.UUID(), nullable=False, comment='Foreign key to UnifiedSchema'),
    sa.Column('batch_name', sa.String(length=255), nullable=False, comment='Descriptive name for the migration batch'),
    sa.Column('started_at', sa.DateTime(timezone=True), nullable=True, comment='Migration start timestamp'),
    sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True, comment='Migration completion timestamp'),
    sa.Column('status', sa.String(length=20), nullable=False, comment='Current batch status'),
    sa.Column('total_files', sa.Integer(), nullable=False, comment='Number of files in the batch'),
    sa.Column('processed_files', sa.Integer(), nullable=False, comment='Number of successfully processed files'),
    sa.Column('failed_files', sa.Integer(), nullable=False, comment='Number of failed files'),
    sa.Column('total_records', sa.Integer(), nullable=False, comment='Total records migrated'),
    sa.Column('error_summary', postgresql.JSON(astext_type=Text()), nullable=True, comment='Summary of errors encountered'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("status IN ('pending', 'running', 'completed', 'failed', 'rolled_back')", name=op.f('ck_migration_batch_valid_status')),
    sa.CheckConstraint('failed_files >= 0', name=op.f('ck_migration_batch_valid_failed_files')),
    sa.CheckConstraint('processed_files + failed_files <= total_files', name=op.f('ck_migration_batch_valid_file_counts')),
    sa.CheckConstraint('processed_files >= 0', name=op.f('ck_migration_batch_valid_processed_files')),
    sa.CheckConstraint('total_files >= 0', name=op.f('ck_migration_batch_valid_total_files')),
    sa.CheckConstraint('total_records >= 0', name=op.f('ck_migration_batch_valid_total_records')),
    sa.ForeignKeyConstraint(['unified_schema_id'], ['unified_schema.id'], name=op.f('fk_migration_batch_unified_schema_id_unified_schema'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_migration_batch'))
    )
    op.create_table('table_definition',
    sa.Column('unified_schema_id', sa.UUID(), nullable=False, comment='Foreign key to UnifiedSchema'),
    sa.Column('table_name', sa.String(length=100), nullable=False, comment='Name of the database table'),
    sa.Column('display_name', sa.String(length=255), nullable=True, comment='Human-readable table name'),
    sa.Column('description', sa.Text(), nullable=True, comment='Description of table purpose'),
    sa.Column('column_definitions', postgresql.JSON(astext_type=Text()), nullable=False, comment='Column definitions as JSON'),
    sa.Column('primary_key_columns', postgresql.JSON(astext_type=Text()), nullable=True, comment='List of primary key column names'),
    sa.Column('indexes', postgresql.JSON(astext_type=Text()), nullable=True, comment='Index definitions as JSON'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint('LENGTH(table_name) > 0', name=op.f('ck_table_definition_valid_table_name')),
    sa.ForeignKeyConstraint(['unified_schema_id'], ['unified_schema.id'], name=op.f('fk_table_definition_unified_schema_id_unified_schema'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_table_definition'))
    )
    op.create_table('column_definition',
    sa.Column('excel_sheet_id', sa.UUID(), nullable=False, comment='Foreign key to ExcelSheet'),
    sa.Column('column_name', sa.String(length=255), nullable=False, comment='Original column name from Excel'),
    sa.Column('column_index', sa.Integer(), nullable=False, comment='Zero-based column position'),
    sa.Column('normalized_name', sa.String(length=255), nullable=True, comment='Standardized column name for database'),
    sa.Column('excel_data_type', sa.String(length=20), nullable=True, comment='Excel data type classification'),
    sa.Column('sql_data_type', sa.String(length=20), nullable=True, comment='Target SQL data type'),
    sa.Column('max_length', sa.Integer(), nullable=True, comment='Maximum observed string length for VARCHAR sizing'),
    sa.Column('precision', sa.Integer(), nullable=True, comment='Decimal precision for numeric types'),
    sa.Column('scale', sa.Integer(), nullable=True, comment='Decimal scale for numeric types'),
    sa.Column('nullable', sa.Boolean(), nullable=False, comment='Whether column allows NULL values'),
    sa.Column('has_duplicates', sa.Boolean(), nullable=True, comment='Whether column contains duplicate values'),
    sa.Column('unique_count', sa.Integer(), nullable=True, comment='Number of unique values in column'),
    sa.Column('null_count', sa.Integer(), nullable=True, comment='Number of NULL/empty values'),
    sa.Column('sample_values', postgresql.JSON(astext_type=Text()), nullable=True, comment='Array of sample values for validation'),
    sa.Column('mapping_confidence', sa.Float(), nullable=True, comment='Confidence score for type inference (0.0-1.0)'),
    sa.Column('inference_rules_applied', postgresql.JSON(astext_type=Text()), nullable=True, comment='List of type inference rules that were applied'),
    sa.Column('is_oews_standard', sa.Boolean(), nullable=True, comment='Whether this is a standard OEWS column'),
    sa.Column('oews_column_type', sa.String(length=50), nullable=True, comment='OEWS column classification: geographic, occupation, industry, wage, employment'),
    sa.Column('has_special_values', sa.Boolean(), nullable=True, comment="Whether column contains OEWS special values like '#' or '*'"),
    sa.Column('special_values_count', sa.Integer(), nullable=True, comment='Count of special/suppressed values'),
    sa.Column('validation_rules', postgresql.JSON(astext_type=Text()), nullable=True, comment='Custom validation rules for this column'),
    sa.Column('validation_errors', sa.Text(), nullable=True, comment='Any validation errors found in column data'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("excel_data_type IS NULL OR excel_data_type IN ('text', 'numeric', 'date', 'boolean', 'formula')", name=op.f('ck_column_definition_valid_excel_data_type')),
    sa.CheckConstraint("oews_column_type IS NULL OR oews_column_type IN ('geographic', 'occupation', 'industry', 'wage', 'employment', 'metadata', 'other')", name=op.f('ck_column_definition_valid_oews_column_type')),
    sa.CheckConstraint("sql_data_type IS NULL OR sql_data_type IN ('varchar', 'text', 'integer', 'bigint', 'decimal', 'float', 'date', 'datetime', 'timestamp', 'boolean', 'json')", name=op.f('ck_column_definition_valid_sql_data_type')),
    sa.CheckConstraint('column_index >= 0', name=op.f('ck_column_definition_valid_column_index')),
    sa.CheckConstraint('mapping_confidence IS NULL OR (mapping_confidence >= 0.0 AND mapping_confidence <= 1.0)', name=op.f('ck_column_definition_valid_mapping_confidence')),
    sa.CheckConstraint('max_length IS NULL OR max_length > 0', name=op.f('ck_column_definition_valid_max_length')),
    sa.CheckConstraint('null_count IS NULL OR null_count >= 0', name=op.f('ck_column_definition_valid_null_count')),
    sa.CheckConstraint('precision IS NULL OR precision > 0', name=op.f('ck_column_definition_valid_precision')),
    sa.CheckConstraint('scale IS NULL OR scale >= 0', name=op.f('ck_column_definition_valid_scale')),
    sa.CheckConstraint('special_values_count IS NULL OR special_values_count >= 0', name=op.f('ck_column_definition_valid_special_values_count')),
    sa.CheckConstraint('unique_count IS NULL OR unique_count >= 0', name=op.f('ck_column_definition_valid_unique_count')),
    sa.ForeignKeyConstraint(['excel_sheet_id'], ['excel_sheet.id'], name=op.f('fk_column_definition_excel_sheet_id_excel_sheet'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_column_definition'))
    )
    op.create_table('migration_record',
    sa.Column('migration_batch_id', sa.UUID(), nullable=False, comment='Foreign key to MigrationBatch'),
    sa.Column('excel_file_id', sa.UUID(), nullable=False, comment='Foreign key to ExcelFile'),
    sa.Column('started_at', sa.DateTime(timezone=True), nullable=True, comment='File migration start timestamp'),
    sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True, comment='File migration completion timestamp'),
    sa.Column('status', sa.String(length=20), nullable=False, comment='Current migration status'),
    sa.Column('records_processed', sa.Integer(), nullable=False, comment='Number of records migrated from this file'),
    sa.Column('records_skipped', sa.Integer(), nullable=False, comment='Number of invalid/duplicate records skipped'),
    sa.Column('records_failed', sa.Integer(), nullable=False, comment='Number of records that failed migration'),
    sa.Column('validation_errors', postgresql.JSON(astext_type=Text()), nullable=True, comment='Detailed validation error information'),
    sa.Column('rollback_data', postgresql.JSON(astext_type=Text()), nullable=True, comment='Information needed for rollback operations'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("status IN ('pending', 'analyzing', 'migrating', 'completed', 'failed', 'rolled_back')", name=op.f('ck_migration_record_valid_status')),
    sa.CheckConstraint('records_failed >= 0', name=op.f('ck_migration_record_valid_records_failed')),
    sa.CheckConstraint('records_processed >= 0', name=op.f('ck_migration_record_valid_records_processed')),
    sa.CheckConstraint('records_skipped >= 0', name=op.f('ck_migration_record_valid_records_skipped')),
    sa.ForeignKeyConstraint(['excel_file_id'], ['excel_file.id'], name=op.f('fk_migration_record_excel_file_id_excel_file'), ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['migration_batch_id'], ['migration_batch.id'], name=op.f('fk_migration_record_migration_batch_id_migration_batch'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_migration_record'))
    )
    op.create_table('column_mapping',
    sa.Column('column_definition_id', sa.UUID(), nullable=False, comment='Foreign key to source ColumnDefinition'),
    sa.Column('table_definition_id', sa.UUID(), nullable=False, comment='Foreign key to target TableDefinition'),
    sa.Column('target_column_name', sa.String(length=255), nullable=False, comment='Target database column name'),
    sa.Column('transformation_rule', sa.Text(), nullable=True, comment='Data transformation rule (SQL expression or function)'),
    sa.Column('is_active', sa.Boolean(), nullable=False, comment='Whether this mapping is currently active'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.ForeignKeyConstraint(['column_definition_id'], ['column_definition.id'], name=op.f('fk_column_mapping_column_definition_id_column_definition'), ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['table_definition_id'], ['table_definition.id'], name=op.f('fk_column_mapping_table_definition_id_table_definition'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_column_mapping'))
    )
    op.create_table('validation_report',
    sa.Column('migration_record_id', sa.UUID(), nullable=False, comment='Foreign key to MigrationRecord'),
    sa.Column('validation_type', sa.String(length=50), nullable=False, comment='Type of validation: schema, data_integrity, referential_integrity, business_rules'),
    sa.Column('start_time', sa.DateTime(timezone=True), nullable=False, comment='Validation start timestamp'),
    sa.Column('end_time', sa.DateTime(timezone=True), nullable=True, comment='Validation completion timestamp'),
    sa.Column('validation_status', sa.String(length=20), nullable=False, comment='Validation result status'),
    sa.Column('total_records_validated', sa.Integer(), nullable=False, comment='Number of records validated'),
    sa.Column('total_errors', sa.Integer(), nullable=False, comment='Number of validation errors found'),
    sa.Column('total_warnings', sa.Integer(), nullable=False, comment='Number of validation warnings found'),
    sa.Column('data_integrity_score', sa.Float(), nullable=True, comment='Data integrity score (0.0-1.0)'),
    sa.Column('errors_by_type', postgresql.JSON(astext_type=Text()), nullable=True, comment='Error counts by category'),
    sa.Column('validation_errors', postgresql.JSON(astext_type=Text()), nullable=True, comment='Detailed validation error list'),
    sa.Column('recommendations', postgresql.JSON(astext_type=Text()), nullable=True, comment='Recommendations for data quality improvement'),
    sa.Column('summary', sa.Text(), nullable=True, comment='Human-readable validation summary'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record creation timestamp'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False, comment='Record last update timestamp'),
    sa.CheckConstraint("validation_status IN ('pending', 'running', 'passed', 'failed', 'warning')", name=op.f('ck_validation_report_valid_validation_status')),
    sa.CheckConstraint("validation_type IN ('schema', 'data_integrity', 'referential_integrity', 'business_rules')", name=op.f('ck_validation_report_valid_validation_type')),
    sa.CheckConstraint('data_integrity_score IS NULL OR (data_integrity_score >= 0.0 AND data_integrity_score <= 1.0)', name=op.f('ck_validation_report_valid_data_integrity_score')),
    sa.CheckConstraint('total_errors >= 0', name=op.f('ck_validation_report_valid_total_errors')),
    sa.CheckConstraint('total_records_validated >= 0', name=op.f('ck_validation_report_valid_total_records_validated')),
    sa.CheckConstraint('total_warnings >= 0', name=op.f('ck_validation_report_valid_total_warnings')),
    sa.ForeignKeyConstraint(['migration_record_id'], ['migration_record.id'], name=op.f('fk_validation_report_migration_record_id_migration_record'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('pk_validation_report'))
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('validation_report')
    op.drop_table('column_mapping')
    op.drop_table('migration_record')
    op.drop_table('column_definition')
    op.drop_table('table_definition')
    op.drop_table('migration_batch')
    op.drop_table('excel_sheet')
    op.drop_table('unified_schema')
    op.drop_table('excel_file')
    op.drop_table('audit_log')
    # ### end Alembic commands ###
